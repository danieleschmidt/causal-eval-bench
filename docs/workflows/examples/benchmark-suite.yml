# Comprehensive Benchmark Suite Workflow
# This workflow runs performance benchmarks and tracks regression

name: Benchmark Suite

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Run nightly benchmarks
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'full'
        type: choice
        options:
          - unit
          - integration
          - performance
          - load
          - full

jobs:
  unit-benchmarks:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.benchmark_type == 'unit' || github.event.inputs.benchmark_type == 'full' || github.event_name != 'workflow_dispatch' }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install poetry
          poetry install --with test

      - name: Run unit test benchmarks
        run: |
          poetry run pytest tests/unit/ \
            --benchmark-only \
            --benchmark-json=benchmark-unit.json \
            --benchmark-sort=mean \
            --benchmark-columns=min,max,mean,stddev,ops \
            --benchmark-group-by=group

      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: benchmark-unit.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          comment-on-alert: true
          alert-threshold: '150%'
          fail-on-alert: false

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: unit-benchmarks
          path: benchmark-unit.json

  integration-benchmarks:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.benchmark_type == 'integration' || github.event.inputs.benchmark_type == 'full' || github.event_name != 'workflow_dispatch' }}
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: causal_eval_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install poetry
          poetry install --with test

      - name: Run database migrations
        run: |
          poetry run alembic upgrade head
        env:
          DATABASE_URL: postgresql://postgres:testpass@localhost:5432/causal_eval_test

      - name: Run integration benchmarks
        run: |
          poetry run pytest tests/integration/ \
            --benchmark-only \
            --benchmark-json=benchmark-integration.json \
            --benchmark-sort=mean
        env:
          DATABASE_URL: postgresql://postgres:testpass@localhost:5432/causal_eval_test
          REDIS_URL: redis://localhost:6379

      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: benchmark-integration.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true

  performance-benchmarks:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.benchmark_type == 'performance' || github.event.inputs.benchmark_type == 'full' || github.event_name != 'workflow_dispatch' }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install poetry
          poetry install --with test

      - name: Run performance benchmarks
        run: |
          poetry run pytest tests/performance/ \
            --benchmark-json=benchmark-performance.json \
            --benchmark-columns=min,max,mean,stddev,ops,rounds \
            --benchmark-group-by=func \
            --benchmark-sort=mean

      - name: Generate performance report
        run: |
          poetry run python scripts/generate_performance_report.py \
            --input benchmark-performance.json \
            --output performance-report.html

      - name: Upload performance report
        uses: actions/upload-artifact@v4
        with:
          name: performance-report
          path: |
            benchmark-performance.json
            performance-report.html

  load-testing:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.benchmark_type == 'load' || github.event.inputs.benchmark_type == 'full' || github.event_name == 'schedule' }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build test image
        uses: docker/build-push-action@v5
        with:
          context: .
          target: development
          load: true
          tags: causal-eval:test
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Start application stack
        run: |
          docker-compose -f docker-compose.yml up -d
          sleep 30  # Wait for services to be ready

      - name: Wait for application to be ready
        run: |
          timeout 60 bash -c 'until curl -f http://localhost:8000/health; do sleep 2; done'

      - name: Run load tests
        run: |
          docker-compose run --rm performance-test \
            locust -f tests/load/locustfile.py \
            --host http://app:8000 \
            --users 50 \
            --spawn-rate 5 \
            --run-time 300s \
            --headless \
            --html load-test-report.html \
            --csv load-test-results

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results
          path: |
            load-test-report.html
            load-test-results_*

      - name: Stop application stack
        if: always()
        run: docker-compose down -v

  memory-profiling:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.benchmark_type == 'full' || github.event_name == 'schedule' }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install poetry
          poetry install --with test
          poetry add memory-profiler

      - name: Run memory profiling
        run: |
          poetry run python -m memory_profiler scripts/memory_profile_test.py > memory-profile.txt

      - name: Generate memory report
        run: |
          echo "## Memory Profiling Report" > memory-report.md
          echo "Generated on: $(date)" >> memory-report.md
          echo "" >> memory-report.md
          echo "```" >> memory-report.md
          cat memory-profile.txt >> memory-report.md
          echo "```" >> memory-report.md

      - name: Upload memory profile
        uses: actions/upload-artifact@v4
        with:
          name: memory-profile
          path: |
            memory-profile.txt
            memory-report.md

  regression-analysis:
    runs-on: ubuntu-latest
    needs: [unit-benchmarks, integration-benchmarks, performance-benchmarks]
    if: always() && (github.event_name == 'schedule' || github.event.inputs.benchmark_type == 'full')
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download all benchmark artifacts
        uses: actions/download-artifact@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install analysis dependencies
        run: |
          pip install pandas numpy matplotlib seaborn

      - name: Analyze performance trends
        run: |
          python scripts/analyze_performance_trends.py \
            --benchmark-dir . \
            --output regression-analysis.html \
            --format html

      - name: Check for regressions
        id: regression-check
        run: |
          python scripts/check_regressions.py \
            --threshold 20 \
            --output regression-summary.json

      - name: Create regression issue
        if: steps.regression-check.outputs.regressions-found == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = JSON.parse(fs.readFileSync('regression-summary.json'));
            
            const body = `## Performance Regression Detected
            
            The following performance regressions were detected:
            
            ${summary.regressions.map(r => 
              `- **${r.test}**: ${r.change}% slower than baseline`
            ).join('\n')}
            
            **Commit**: ${context.sha}
            **Workflow**: ${context.workflow}
            **Run**: ${context.runNumber}
            `;
            
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Performance Regression - ${new Date().toISOString().split('T')[0]}`,
              body: body,
              labels: ['performance', 'regression', 'bug']
            });

      - name: Upload regression analysis
        uses: actions/upload-artifact@v4
        with:
          name: regression-analysis
          path: |
            regression-analysis.html
            regression-summary.json

  benchmark-summary:
    runs-on: ubuntu-latest
    needs: [unit-benchmarks, integration-benchmarks, performance-benchmarks, load-testing, memory-profiling]
    if: always()
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4

      - name: Generate benchmark summary
        run: |
          echo "# Benchmark Summary - $(date)" > benchmark-summary.md
          echo "" >> benchmark-summary.md
          
          if [ -f unit-benchmarks/benchmark-unit.json ]; then
            echo "## Unit Test Benchmarks" >> benchmark-summary.md
            echo "✅ Completed" >> benchmark-summary.md
          fi
          
          if [ -f integration-benchmarks/benchmark-integration.json ]; then
            echo "## Integration Benchmarks" >> benchmark-summary.md
            echo "✅ Completed" >> benchmark-summary.md
          fi
          
          if [ -f performance-report/benchmark-performance.json ]; then
            echo "## Performance Benchmarks" >> benchmark-summary.md
            echo "✅ Completed" >> benchmark-summary.md
          fi
          
          if [ -f load-test-results/load-test-report.html ]; then
            echo "## Load Testing" >> benchmark-summary.md
            echo "✅ Completed" >> benchmark-summary.md
          fi

      - name: Comment PR with summary
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('benchmark-summary.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });