[tool.poetry]
name = "causal-eval-bench"
version = "0.1.0"
description = "Comprehensive evaluation framework for testing genuine causal reasoning in language models"
authors = ["Daniel Schmidt <daniel@terragon-labs.com>"]
license = "MIT"
readme = "README.md"
homepage = "https://github.com/your-org/causal-eval-bench"
repository = "https://github.com/your-org/causal-eval-bench"
documentation = "https://docs.causal-eval-bench.org"
keywords = ["causal-reasoning", "evaluation", "benchmark", "language-models", "ai"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Science/Research",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]
packages = [{include = "causal_eval"}]

[tool.poetry.dependencies]
python = "^3.9"
fastapi = "^0.116.1"
uvicorn = {extras = ["standard"], version = "^0.35.0"}
pydantic = "^2.5.0"
sqlalchemy = "^2.0.0"
alembic = "^1.13.0"
asyncpg = "^0.29.0"
redis = "^5.0.0"
httpx = "^0.25.0"
aiofiles = "^24.1.0"
python-multipart = "^0.0.6"
python-jose = {extras = ["cryptography"], version = "^3.3.0"}
passlib = {extras = ["bcrypt"], version = "^1.7.4"}
typer = "^0.9.0"
rich = "^13.7.0"
pandas = "^2.1.0"
numpy = "^2.0.2"
scipy = "^1.11.0"
scikit-learn = "^1.3.0"
matplotlib = "^3.8.0"
seaborn = "^0.13.0"
plotly = "^6.2.0"
jinja2 = "^3.1.2"
pyyaml = "^6.0.1"
python-dotenv = "^1.0.0"
loguru = "^0.7.2"
structlog = "^23.2.0"
prometheus-client = "^0.22.1"
sentry-sdk = {extras = ["fastapi"], version = "^1.38.0"}
openai = "^1.3.0"
anthropic = "^0.60.0"
transformers = "^4.35.0"
torch = {version = "^2.1.0", optional = true}
sentence-transformers = "^5.0.0"
nltk = "^3.8.1"
spacy = "^3.7.0"
datasets = "^4.0.0"

[tool.poetry.group.dev.dependencies]
pytest = "^8.4.1"
pytest-asyncio = "^1.1.0"
pytest-cov = "^6.2.1"
pytest-mock = "^3.12.0"
pytest-xdist = "^3.5.0"
pytest-benchmark = "^5.1.0"
hypothesis = "^6.88.0"
factory-boy = "^3.3.0"
faker = "^37.5.3"
freezegun = "^1.2.2"
responses = "^0.25.7"
httpx = "^0.25.0"
respx = "^0.20.0"

[tool.poetry.group.lint.dependencies]
black = "^25.1.0"
isort = "^6.0.1"
ruff = "^0.12.5"
mypy = "^1.7.0"
bandit = "^1.7.5"
safety = "^3.2.13"
pre-commit = "^4.2.0"

[tool.poetry.group.docs.dependencies]
mkdocs = "^1.5.0"
mkdocs-material = "^9.4.0"
mkdocs-mermaid2-plugin = "^1.1.0"
mkdocstrings = {extras = ["python"], version = "^0.30.0"}
sphinx = "^7.2.0"
sphinx-rtd-theme = "^1.3.0"
sphinx-autodoc-typehints = "^1.25.0"

[tool.poetry.group.test.dependencies]
coverage = {extras = ["toml"], version = "^7.3.0"}
pytest-html = "^4.1.0"
pytest-json-report = "^1.5.0"
locust = "^2.17.0"

[tool.poetry.extras]
torch = ["torch"]
all = ["torch"]

[tool.poetry.scripts]
causal-eval = "causal_eval.cli.main:app"
causal-eval-server = "causal_eval.api.main:run_server"
causal-eval-worker = "causal_eval.worker.main:main"

[tool.poetry.plugins."causal_eval.tasks"]
causal_attribution = "causal_eval.tasks.attribution:CausalAttribution"
counterfactual = "causal_eval.tasks.counterfactual:CounterfactualReasoning"
intervention = "causal_eval.tasks.intervention:CausalIntervention"
causal_chain = "causal_eval.tasks.chain:CausalChain"
confounding = "causal_eval.tasks.confounding:ConfoundingAnalysis"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

# =============================================================================
# TOOL CONFIGURATIONS
# =============================================================================

[tool.black]
line-length = 88
target-version = ['py39']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.isort]
profile = "black"
multi_line_output = 3
include_trailing_comma = true
force_grid_wrap = 0
use_parentheses = true
ensure_newline_before_comments = true
line_length = 88
skip_gitignore = true
known_first_party = ["causal_eval"]
known_third_party = [
    "fastapi",
    "pydantic",
    "sqlalchemy",
    "redis",
    "httpx",
    "typer",
    "rich",
    "pandas",
    "numpy",
    "scipy",
    "sklearn",
    "matplotlib",
    "seaborn",
    "plotly",
]

[tool.ruff]
line-length = 88
target-version = "py39"
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort
    "C",   # flake8-comprehensions
    "B",   # flake8-bugbear
    "UP",  # pyupgrade
    "N",   # pep8-naming
    "S",   # bandit security
    "A",   # flake8-builtins
    "COM", # flake8-commas
    "DTZ", # flake8-datetimez
    "EM",  # flake8-errmsg
    "ISC", # flake8-implicit-str-concat
    "PIE", # flake8-pie
    "PT",  # flake8-pytest-style
    "Q",   # flake8-quotes
    "RSE", # flake8-raise
    "RET", # flake8-return
    "SIM", # flake8-simplify
    "TCH", # flake8-type-checking
    "ARG", # flake8-unused-arguments
    "PTH", # flake8-use-pathlib
    "ERA", # eradicate
    "PL",  # pylint
    "TRY", # tryceratops
    "FLY", # flynt
    "PERF", # perflint
    "FURB", # refurb
    "RUF", # ruff-specific rules
]
ignore = [
    "E501",   # line too long, handled by black
    "B008",   # do not perform function calls in argument defaults
    "C901",   # too complex
    "S101",   # use of assert
    "S104",   # hardcoded bind all interfaces
    "PLR0913", # too many arguments
    "PLR0915", # too many statements
    "PLR2004", # magic value used in comparison
    "TRY003",  # avoid specifying long messages in exception
]
unfixable = ["B"]

[tool.ruff.per-file-ignores]
"tests/**/*" = ["S101", "PLR2004", "ARG001", "ARG002"]
"scripts/**/*" = ["T201", "S602", "S603"]
"docs/**/*" = ["INP001"]

[tool.mypy]
python_version = "3.9"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
show_error_codes = true
show_column_numbers = true
show_error_context = true
pretty = true

[[tool.mypy.overrides]]
module = [
    "anthropic.*",
    "transformers.*",
    "torch.*",
    "sentence_transformers.*",
    "nltk.*",
    "spacy.*",
    "datasets.*",
    "plotly.*",
    "seaborn.*",
    "sklearn.*",
    "scipy.*",
]
ignore_missing_imports = true

[tool.pytest.ini_options]
minversion = "7.0"
addopts = [
    "-ra",
    "--strict-markers",
    "--strict-config",
    "--cov=causal_eval",
    "--cov-report=term-missing",
    "--cov-report=html",
    "--cov-report=xml",
    "--cov-fail-under=80",
]
testpaths = ["tests"]
filterwarnings = [
    "ignore::UserWarning",
    "ignore::DeprecationWarning",
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
    "api: marks tests for API endpoints",
    "evaluation: marks tests for evaluation logic",
    "generation: marks tests for test generation",
]

[tool.coverage.run]
source = ["causal_eval"]
omit = [
    "*/tests/*",
    "*/test_*",
    "*/__pycache__/*",
    "*/migrations/*",
    "*/venv/*",
    "*/.venv/*",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if settings.DEBUG",
    "raise AssertionError",
    "raise NotImplementedError",
    "if 0:",
    "if __name__ == .__main__.:",
    "class .*\\bProtocol\\):",
    "@(abc\\.)?abstractmethod",
]

[tool.bandit]
exclude_dirs = ["tests"]
skips = ["B101", "B601"]

[tool.bandit.assert_used]
skips = ["*_test.py", "*/test_*.py"]